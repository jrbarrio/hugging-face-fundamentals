{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8ce09b2",
   "metadata": {},
   "source": [
    "# Computer vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6ede97",
   "metadata": {},
   "source": [
    "## Image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69694a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"nlphuji/flickr30k\")\n",
    "image = dataset['test'][134][\"image\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd37f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(image) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18567cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"image-classification\", \"google/mobilenet_v2_1.0_224\")\n",
    "pred = pipe(image) \n",
    "print(\"Predicted class:\", pred[0]['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573d7d2c",
   "metadata": {},
   "source": [
    "## Object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a795d3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"object-detection\", \"facebook/detr-resnet-50\", revision=\"no_timm\")\n",
    "outputs = pipe(image, threshold=0.95)\n",
    "\n",
    "for obj in outputs:\n",
    "  box = obj['box'] \n",
    "  print(f\"Detected {obj['label']} with confidence {obj['score']:.2f} at ({box['xmin']}, {box['ymin']}) to ({box['xmax']}, {box['ymax']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4565911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "ax = plt.gca()\n",
    "colors = ['r', 'g', 'b', 'y', 'm', 'c', 'k']\n",
    "plt.imshow(image)  \n",
    "for n, obj in enumerate(outputs):\n",
    "  box = obj['box']   \n",
    "  rect = patches.Rectangle(\n",
    "    (box['xmin'], box['ymin']),\n",
    "    box['xmax']-box['xmin'],\n",
    "    box['ymax']-box['ymin'],\n",
    "    linewidth=1,\n",
    "    edgecolor=colors[n],\n",
    "    facecolor='none')  \n",
    "  ax.add_patch(rect)  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9484415e",
   "metadata": {},
   "source": [
    "## Image background removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab68245",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"image-segmentation\", model=\"briaai/RMBG-1.4\", trust_remote_code=True)\n",
    "outputs = pipe(image)\n",
    "\n",
    "plt.imshow(outputs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e4ace6",
   "metadata": {},
   "source": [
    "# Fine-tunning computer vision models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d98263",
   "metadata": {},
   "source": [
    "## CV fine-tunning: dataset prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc30c655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ideepankarsharma2003/Midjourney_v6_Classification_small_shuffled\")['train']\n",
    "data_splits = dataset.train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42f096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor \n",
    "\n",
    "checkpoint = \"google/mobilenet_v2_1.0_224\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "transform = Compose([ToTensor(), normalize])\n",
    "\n",
    "def transforms(examples):\n",
    "  examples[\"pixel_values\"] = [transform(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "  del examples[\"image\"]\n",
    "  return examples\n",
    "\n",
    "dataset = dataset.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c55b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(dataset[\"train\"][0][\"pixel_values\"].permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829a21e1",
   "metadata": {},
   "source": [
    "## CV fine-tunning: model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412ae7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data_splits[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072dedaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification \n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b914ea0e",
   "metadata": {},
   "source": [
    "## CV fine-tunning: trainer configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a923f8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"dataset_finetune\",\n",
    "  learning_rate=6e-5,\n",
    "  gradient_accumulation_steps=4,\n",
    "  num_train_epochs=3,\n",
    "  push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72f32ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()\n",
    "trainer = Trainer(  \n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=dataset[\"train\"],\n",
    "  eval_dataset=dataset[\"test\"],\n",
    "  processing_class=image_processor,\n",
    "  data_collator=data_collator\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging-face-fundamentals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
