{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8ce09b2",
   "metadata": {},
   "source": [
    "# Computer vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6ede97",
   "metadata": {},
   "source": [
    "## Image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69694a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"nlphuji/flickr30k\")\n",
    "image = dataset['test'][134][\"image\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd37f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(image) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18567cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"image-classification\", \"google/mobilenet_v2_1.0_224\")\n",
    "pred = pipe(image) \n",
    "print(\"Predicted class:\", pred[0]['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573d7d2c",
   "metadata": {},
   "source": [
    "## Object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a795d3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"object-detection\", \"facebook/detr-resnet-50\", revision=\"no_timm\")\n",
    "outputs = pipe(image, threshold=0.95)\n",
    "\n",
    "for obj in outputs:\n",
    "  box = obj['box'] \n",
    "  print(f\"Detected {obj['label']} with confidence {obj['score']:.2f} at ({box['xmin']}, {box['ymin']}) to ({box['xmax']}, {box['ymax']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4565911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "ax = plt.gca()\n",
    "colors = ['r', 'g', 'b', 'y', 'm', 'c', 'k']\n",
    "plt.imshow(image)  \n",
    "for n, obj in enumerate(outputs):\n",
    "  box = obj['box']   \n",
    "  rect = patches.Rectangle(\n",
    "    (box['xmin'], box['ymin']),\n",
    "    box['xmax']-box['xmin'],\n",
    "    box['ymax']-box['ymin'],\n",
    "    linewidth=1,\n",
    "    edgecolor=colors[n],\n",
    "    facecolor='none')  \n",
    "  ax.add_patch(rect)  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9484415e",
   "metadata": {},
   "source": [
    "## Image background removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab68245",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"image-segmentation\", model=\"briaai/RMBG-1.4\", trust_remote_code=True)\n",
    "outputs = pipe(image)\n",
    "\n",
    "plt.imshow(outputs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e4ace6",
   "metadata": {},
   "source": [
    "# Fine-tunning computer vision models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d98263",
   "metadata": {},
   "source": [
    "## CV fine-tunning: dataset prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc30c655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ideepankarsharma2003/Midjourney_v6_Classification_small_shuffled\")['train']\n",
    "data_splits = dataset.train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42f096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor \n",
    "\n",
    "checkpoint = \"google/mobilenet_v2_1.0_224\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "transform = Compose([ToTensor(), normalize])\n",
    "\n",
    "def transforms(examples):\n",
    "  examples[\"pixel_values\"] = [transform(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "  del examples[\"image\"]\n",
    "  return examples\n",
    "\n",
    "dataset = dataset.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c55b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(dataset[\"train\"][0][\"pixel_values\"].permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829a21e1",
   "metadata": {},
   "source": [
    "## CV fine-tunning: model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412ae7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data_splits[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072dedaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification \n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b914ea0e",
   "metadata": {},
   "source": [
    "## CV fine-tunning: trainer configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a923f8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"dataset_finetune\",\n",
    "  learning_rate=6e-5,\n",
    "  gradient_accumulation_steps=4,\n",
    "  num_train_epochs=3,\n",
    "  push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72f32ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()\n",
    "trainer = Trainer(  \n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=dataset[\"train\"],\n",
    "  eval_dataset=dataset[\"test\"],\n",
    "  processing_class=image_processor,\n",
    "  data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce2c641",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(dataset[\"test\"])\n",
    "predictions.metrics[\"test_accuracy\"]\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a339a4d2",
   "metadata": {},
   "source": [
    "# Speech recognition and audio generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cddf2e",
   "metadata": {},
   "source": [
    "## Automatic speech recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf02182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc047093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "\n",
    "dataset = load_dataset(\"CSTR-Edinburgh/vctk\")[\"train\"]\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "sample = dataset[0][\"audio\"]\n",
    "input_preprocessed = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\")\n",
    "predicted_ids = model.generate(input_preprocessed.input_features)\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e829e27a",
   "metadata": {},
   "source": [
    "## Creating speech embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8988d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from speechbrain.inference.speaker import EncoderClassifier\n",
    "\n",
    "speaker_model = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-xvect-voxceleb\")\n",
    "speaker_embeddings = speaker_model.encode_batch(torch.tensor(dataset[0][\"audio\"][\"array\"]))\n",
    "speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf71ef48",
   "metadata": {},
   "source": [
    "## Audio denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4b9305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SpeechT5Processor, SpeechT5ForSpeechToSpeech, SpeechT5HifiGan\n",
    "\n",
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_vc\")\n",
    "model = SpeechT5ForSpeechToSpeech.from_pretrained(\"microsoft/speecht5_vc\")\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9dc16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(audio=dataset[0][\"audio\"], sampling_rate=dataset[0][\"audio\"][\"sampling_rate\"], return_tensors=\"pt\")\n",
    "speech = model.generate_speech(inputs[\"input_values\"], speaker_embeddings, vocoder=vocoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944262ed",
   "metadata": {},
   "source": [
    "# Fine-tuning text to speech models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116ebf00",
   "metadata": {},
   "source": [
    "## Fine-tuning a text to speech model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c574b520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"facebook/voxpopuli\", \"it\", split=\"train\", trust_remote_code=True)\n",
    "print(dataset.features)\n",
    "\n",
    "speaker_model = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-xvect-voxceleb\", savedir=\"pretrained_models/spkrec-xvect-voxceleb\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27968c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SpeechT5Processor\n",
    "\n",
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "def prepare_dataset(example):\n",
    "    audio = example[\"audio\"]\n",
    "    example = processor(text=example[\"normalized_text\"], audio_target=audio[\"array\"],\n",
    "                        sampling_rate=audio[\"sampling_rate\"], return_attention_mask=False)\n",
    "    example[\"labels\"] = example[\"labels\"][0]\n",
    "    with torch.no_grad():\n",
    "        speaker_embeddings = speaker_model.encode_batch(torch.tensor(audio[\"array\"]))\n",
    "        speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2)\n",
    "        example[\"speaker_embeddings\"] = speaker_embeddings.squeeze().cpu().numpy()\n",
    "    return example\n",
    "dataset = dataset.map(prepare_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f888d218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    label_names=[\"labels\"],\n",
    "    data_collator=data_collator\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b4859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SpeechT5ForTextToSpeech, Seq2SeqTrainer\n",
    "\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "\n",
    "trainer = Seq2SeqTrainer(args=training_args, model=model, train_dataset=dataset[\"train\"], eval_dataset=dataset[\"test\"], tokenizer=processor)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d241ec12",
   "metadata": {},
   "source": [
    "## Generating new speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf19b34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"se sono italiano posso cantare l'opera lirica\"\n",
    "\n",
    "speaker_embedding = torch.tensor(dataset[5][\"speaker_embeddings\"]).unsqueeze(0)\n",
    "inputs = processor(text=text, return_tensors=\"pt\")\n",
    "speech = model.generate_speech(inputs[\"input_ids\"], speaker_embedding, vocoder=vocoder)\n",
    "\n",
    "make_spectrogram(speech)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging-face-fundamentals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
