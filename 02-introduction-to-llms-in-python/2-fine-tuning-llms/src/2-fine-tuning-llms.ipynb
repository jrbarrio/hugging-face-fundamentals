{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c93881e",
   "metadata": {},
   "source": [
    "# Preparing for fine-tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189ed8d1",
   "metadata": {},
   "source": [
    "## Tokenizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d7aea6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  5930,  1006,  1031,  1005,  1045, 12524,  1045,  2572,  8025,\n",
      "          1011,  3756,  2013,  2026,  2678,  3573,  2138,  1997,  2035,  1996,\n",
      "          6704,  2008,  5129,  2009,  2043,  2009,  2001,  2034,  2207,  1999,\n",
      "          3476,  1012,  1045,  2036,  2657,  2008,  2012,  2034,  2009,  2001,\n",
      "          8243,  2011,  1057,  1012,  1055,  1012,  8205,  2065,  2009,  2412,\n",
      "          2699,  2000,  4607,  2023,  2406,  1010,  3568,  2108,  1037,  5470,\n",
      "          1997,  3152,  2641,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[  101,  5930,  1006,  1031,  1005,  1045,  2293, 16596,  1011, 10882,\n",
      "          1998,  2572,  5627,  2000,  2404,  2039,  2007,  1037,  2843,  1012,\n",
      "         16596,  1011, 10882,  5691,  1013,  2694,  2024,  2788,  2104, 11263,\n",
      "         25848,  1010,  2104,  1011, 12315,  1998, 28947,  1012,  1045,  2699,\n",
      "          2000,  2066,  2023,  1010,  1045,  2428,  2106,  1010,  2021,  2009,\n",
      "          2003,  2000,  2204,  2694, 16596,  1011, 10882,  2004, 17690,  1019,\n",
      "          2003,  2000,  2732,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from datasets import load_dataset \n",
    "\n",
    "train_data = load_dataset(\"imdb\", split=\"train\") \n",
    "train_data = train_data.shard(num_shards=4, index=0) \n",
    "test_data = load_dataset(\"imdb\", split=\"test\") \n",
    "test_data = test_data.shard(num_shards=4, index=0)\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\") \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the data\n",
    "tokenized_training_data = tokenizer(str(train_data[\"text\"]), return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
    "tokenized_test_data = tokenizer(str(test_data[\"text\"]), return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
    "\n",
    "print(tokenized_training_data)\n",
    "print(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda1670b",
   "metadata": {},
   "source": [
    "## Mapping tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc600f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 6250\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Complete the function\n",
    "def tokenize_function(data):\n",
    "    return tokenizer(data[\"text\"], \n",
    "                     return_tensors=\"pt\", \n",
    "                     padding=True, \n",
    "                     truncation=True, \n",
    "                     max_length=64)\n",
    "\n",
    "tokenized_in_batches = train_data.map(tokenize_function, batched=True)\n",
    "\n",
    "print(tokenized_in_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8968ca82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 6250\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Complete the function\n",
    "def tokenize_function(data):\n",
    "    return tokenizer(data[\"text\"], \n",
    "                     return_tensors=\"pt\", \n",
    "                     padding=True, \n",
    "                     truncation=True, \n",
    "                     max_length=64)\n",
    "\n",
    "# Tokenize row by row\n",
    "tokenized_by_row = train_data.map(tokenize_function, batched=False)\n",
    "\n",
    "print(tokenized_by_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eff489",
   "metadata": {},
   "source": [
    "# Fine-tunning through training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a38359",
   "metadata": {},
   "source": [
    "## Setting up training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3702765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer,  TrainingArguments\n",
    "\n",
    "# Set up an instance of TrainingArguments\n",
    "training_args = TrainingArguments(   \n",
    "  output_dir=\"./finetuned\",\n",
    "  eval_strategy=\"epoch\",\n",
    "  num_train_epochs=3,   \n",
    "  learning_rate=2e-5,     \n",
    "  per_device_train_batch_size=8,   \n",
    "  per_device_eval_batch_size=8,\n",
    "  weight_decay=0.01, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca5a5ad",
   "metadata": {},
   "source": [
    "## Setting up the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e3a783f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f0940670b994f2bb63d3afc47bd1409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4129d97ac922475399635a70571a0bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare the datasets properly\n",
    "def tokenize_and_format(examples):\n",
    "    # Tokenize the texts\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64,\n",
    "        return_tensors=None  # We don't want PT tensors yet\n",
    "    )\n",
    "    \n",
    "    # Add labels\n",
    "    tokenized[\"labels\"] = examples[\"label\"]\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Apply the tokenization to our datasets\n",
    "tokenized_train = train_data.map(tokenize_and_format, batched=True)\n",
    "tokenized_test = test_data.map(tokenize_and_format, batched=True)\n",
    "\n",
    "# Remove unnecessary columns and set format\n",
    "tokenized_train = tokenized_train.remove_columns([\"text\"])\n",
    "tokenized_test = tokenized_test.remove_columns([\"text\"])\n",
    "\n",
    "# Set the format of our datasets to PyTorch\n",
    "tokenized_train.set_format(\"torch\")\n",
    "tokenized_test.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92053629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45461/2340947501.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2346' max='2346' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2346/2346 35:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45461/2340947501.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='783' max='2346' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 783/2346 09:55 < 19:52, 1.31 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/.pyenv/versions/hugging-face-fundamentals/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/jorge/.pyenv/versions/hugging-face-fundamentals/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/jorge/.pyenv/versions/hugging-face-fundamentals/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/jorge/.pyenv/versions/hugging-face-fundamentals/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/jorge/.pyenv/versions/hugging-face-fundamentals/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45461/2340947501.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2346' max='2346' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2346/2346 35:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/.pyenv/versions/hugging-face-fundamentals/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/jorge/.pyenv/versions/hugging-face-fundamentals/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/jorge/.pyenv/versions/hugging-face-fundamentals/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/jorge/.pyenv/versions/hugging-face-fundamentals/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/jorge/.pyenv/versions/hugging-face-fundamentals/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2346, training_loss=0.001443281663559498, metrics={'train_runtime': 2134.137, 'train_samples_per_second': 8.786, 'train_steps_per_second': 1.099, 'total_flos': 616666536000000.0, 'train_loss': 0.001443281663559498, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=lambda pred: {\n",
    "        \"accuracy\": (pred.predictions.argmax(-1) == pred.label_ids).mean().item()\n",
    "    }\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56739583",
   "metadata": {},
   "source": [
    "## Using the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "febeeae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Input Text 1: I'd just like to say, I love the product! Thank you!\n",
      "Predicted Label: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input_text = [\"I'd just like to say, I love the product! Thank you!\"]\n",
    "\n",
    "# Tokenize the new data\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Pass the tokenized inputs through the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract the new predictions\n",
    "predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "\n",
    "label_map = {0: \"Low risk\", 1: \"High risk\"}\n",
    "for i, predicted_label in enumerate(predicted_labels):\n",
    "    churn_label = label_map[predicted_label]\n",
    "    print(f\"\\n Input Text {i + 1}: {input_text[i]}\")\n",
    "    print(f\"Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d5afc1",
   "metadata": {},
   "source": [
    "# Fine-tuning approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baff22e0",
   "metadata": {},
   "source": [
    "## Transfer learning with one-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36caebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load GPT-2 model and tokenizer for text generation\n",
    "model_name = \"gpt2\"  # you can also use \"gpt2-medium\" or \"gpt2-large\" for better results\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# We need to set the pad token to the eos token for GPT-2\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c038154",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete response:\n",
      "\n",
      "Text: \"The dinner we had was great and the service too.\"\n",
      "Classify the sentiment of this sentence as either positive or negative.\n",
      "Example:\n",
      "Text: \"The food was delicious\"\n",
      "Sentiment: Positive\n",
      "Text: \"The dinner we had was great and the service too.\"\n",
      "Sentiment: Negative\n",
      "Text: \"The dinner we had was great and the service too.\"\n",
      "Subject: \"\n",
      "\n",
      "Extracted sentiment: Negative\n",
      "Text: \"The dinner we had was great and the service too.\"\n",
      "Subject: \"\n"
     ]
    }
   ],
   "source": [
    "# Include an example in the input text\n",
    "input_text = \"\"\"\n",
    "Text: \"The dinner we had was great and the service too.\"\n",
    "Classify the sentiment of this sentence as either positive or negative.\n",
    "Example:\n",
    "Text: \"The food was delicious\"\n",
    "Sentiment: Positive\n",
    "Text: \"The dinner we had was great and the service too.\"\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Generate the completion\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=len(inputs[\"input_ids\"][0]) + 20,  # Allow space for the answer\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "\n",
    "# Decode and print the result\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Complete response:\")\n",
    "print(generated_text)\n",
    "\n",
    "# Extract just the sentiment part (after the last \"Sentiment:\")\n",
    "sentiment = generated_text.split(\"Sentiment:\")[-1].strip()\n",
    "print(\"\\nExtracted sentiment:\", sentiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging-face-fundamentals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
