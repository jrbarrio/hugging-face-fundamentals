{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c93881e",
   "metadata": {},
   "source": [
    "# Preparing for fine-tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189ed8d1",
   "metadata": {},
   "source": [
    "## Tokenizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d7aea6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  5930,  1006,  1031,  1005,  1045, 12524,  1045,  2572,  8025,\n",
      "          1011,  3756,  2013,  2026,  2678,  3573,  2138,  1997,  2035,  1996,\n",
      "          6704,  2008,  5129,  2009,  2043,  2009,  2001,  2034,  2207,  1999,\n",
      "          3476,  1012,  1045,  2036,  2657,  2008,  2012,  2034,  2009,  2001,\n",
      "          8243,  2011,  1057,  1012,  1055,  1012,  8205,  2065,  2009,  2412,\n",
      "          2699,  2000,  4607,  2023,  2406,  1010,  3568,  2108,  1037,  5470,\n",
      "          1997,  3152,  2641,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[  101,  5930,  1006,  1031,  1005,  1045,  2293, 16596,  1011, 10882,\n",
      "          1998,  2572,  5627,  2000,  2404,  2039,  2007,  1037,  2843,  1012,\n",
      "         16596,  1011, 10882,  5691,  1013,  2694,  2024,  2788,  2104, 11263,\n",
      "         25848,  1010,  2104,  1011, 12315,  1998, 28947,  1012,  1045,  2699,\n",
      "          2000,  2066,  2023,  1010,  1045,  2428,  2106,  1010,  2021,  2009,\n",
      "          2003,  2000,  2204,  2694, 16596,  1011, 10882,  2004, 17690,  1019,\n",
      "          2003,  2000,  2732,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from datasets import load_dataset \n",
    "\n",
    "train_data = load_dataset(\"imdb\", split=\"train\") \n",
    "train_data = train_data.shard(num_shards=4, index=0) \n",
    "test_data = load_dataset(\"imdb\", split=\"test\") \n",
    "test_data = test_data.shard(num_shards=4, index=0)\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\") \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the data\n",
    "tokenized_training_data = tokenizer(str(train_data[\"text\"]), return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
    "tokenized_test_data = tokenizer(str(test_data[\"text\"]), return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
    "\n",
    "print(tokenized_training_data)\n",
    "print(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda1670b",
   "metadata": {},
   "source": [
    "## Mapping tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc600f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf797f8711d49cab74359df9ff025df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 6250\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Complete the function\n",
    "def tokenize_function(data):\n",
    "    return tokenizer(data[\"text\"], \n",
    "                     return_tensors=\"pt\", \n",
    "                     padding=True, \n",
    "                     truncation=True, \n",
    "                     max_length=64)\n",
    "\n",
    "tokenized_in_batches = train_data.map(tokenize_function, batched=True)\n",
    "\n",
    "print(tokenized_in_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8968ca82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c46ab2d272574f43a423d5e3ecfe2df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 6250\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Complete the function\n",
    "def tokenize_function(data):\n",
    "    return tokenizer(data[\"text\"], \n",
    "                     return_tensors=\"pt\", \n",
    "                     padding=True, \n",
    "                     truncation=True, \n",
    "                     max_length=64)\n",
    "\n",
    "# Tokenize row by row\n",
    "tokenized_by_row = train_data.map(tokenize_function, batched=False)\n",
    "\n",
    "print(tokenized_by_row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging-face-fundamentals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
