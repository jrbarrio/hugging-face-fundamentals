{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09ad1c36",
   "metadata": {},
   "source": [
    "# Evaluating LLM performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b136c4",
   "metadata": {},
   "source": [
    "## Loading metrics with evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a4db3801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load the metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49d81ba",
   "metadata": {},
   "source": [
    "## Describing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "98caa856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:\n",
      "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
      " Where:\n",
      "TP: True positive\n",
      "TN: True negative\n",
      "FP: False positive\n",
      "FN: False negative\n",
      "\n",
      "\n",
      "Precision is the fraction of correctly labeled positive examples out of all of the examples that were labeled as positive. It is computed via the equation:\n",
      "Precision = TP / (TP + FP)\n",
      "where TP is the True positives (i.e. the examples correctly labeled as positive) and FP is the False positive examples (i.e. the examples incorrectly labeled as positive).\n",
      "\n",
      "\n",
      "Recall is the fraction of the positive examples that were correctly labeled by the model as positive. It can be computed with the equation:\n",
      "Recall = TP / (TP + FN)\n",
      "Where TP is the true positives and FN is the false negatives.\n",
      "\n",
      "\n",
      "The F1 score is the harmonic mean of the precision and recall. It can be computed with the equation:\n",
      "F1 = 2 * (precision * recall) / (precision + recall)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Obtain a description of each metric\n",
    "print(accuracy.description)\n",
    "print(precision.description)\n",
    "print(recall.description)\n",
    "print(f1.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "650fc347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The required data types for accuracy are: {'predictions': Value('int32'), 'references': Value('int32')}.\n",
      "The required data types for precision are: {'predictions': Value('int32'), 'references': Value('int32')}.\n",
      "The required data types for recall are: {'predictions': Value('int32'), 'references': Value('int32')}.\n",
      "The required data types for f1 are: {'predictions': Value('int32'), 'references': Value('int32')}.\n"
     ]
    }
   ],
   "source": [
    "# See the required data types\n",
    "print(f\"The required data types for accuracy are: {accuracy.features}.\")\n",
    "print(f\"The required data types for precision are: {precision.features}.\")\n",
    "print(f\"The required data types for recall are: {recall.features}.\")\n",
    "print(f\"The required data types for f1 are: {f1.features}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558614aa",
   "metadata": {},
   "source": [
    "## Using evaluate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "08038ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load a pretrained model and tokenizer (this is an example using a sentiment analysis model)\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Prepare some example text data\n",
    "texts = [\"I love this movie!\", \"This was a terrible experience.\", \"The food was okay.\"]\n",
    "\n",
    "# Tokenize the inputs\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Get the model outputs\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Now outputs.logits can be used with torch.argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9c62d8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1]\n",
      "{'accuracy': 1.0}\n",
      "{'precision': 1.0}\n",
      "{'recall': 1.0}\n",
      "{'f1': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "# Extract the new predictions\n",
    "predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "\n",
    "print(predicted_labels)\n",
    "\n",
    "validate_labels = [1, 0, 1]  # Example true labels for the validation set\n",
    "\n",
    "# Compute the metrics by comparing real and predicted labels\n",
    "print(accuracy.compute(references=validate_labels, predictions=predicted_labels))\n",
    "print(precision.compute(references=validate_labels, predictions=predicted_labels))\n",
    "print(recall.compute(references=validate_labels, predictions=predicted_labels))\n",
    "print(f1.compute(references=validate_labels, predictions=predicted_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a9d3ce",
   "metadata": {},
   "source": [
    "# Metrics for language tasks: perplexity and BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de4544b",
   "metadata": {},
   "source": [
    "## Evaluating perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e961b58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:  Current trends show that by 2030, the number of people living in poverty will be at its lowest level\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a700604c4e5e4fa9b31f6788abf62072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  3441.6679486083985\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"  # you can also use \"gpt2-medium\", \"gpt2-large\", or \"gpt2-xl\" for larger models\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "input_text = \"Current trends show that by 2030\"\n",
    "\n",
    "# Encode the input text, generate and decode it\n",
    "input_text_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "output = model.generate(input_text_ids, max_length=20)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Text: \", generated_text)\n",
    "\n",
    "# Load and compute the perplexity score\n",
    "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "results = perplexity.compute(model_id=\"gpt2\", predictions=generated_text)\n",
    "print(\"Perplexity: \", results['mean_perplexity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2e8113",
   "metadata": {},
   "source": [
    "## BLEU translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "647390fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated: Hey, how are you?\n",
      "{'bleu': 0.7598356856515925, 'precisions': [0.8333333333333334, 0.8, 0.75, 0.6666666666666666], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 6, 'reference_length': 6}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import evaluate\n",
    "\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
    "\n",
    "input_sentence_1 = \"Hola, ¿cómo estás?\"\n",
    "\n",
    "reference_1 = [[\"Hello, how are you?\", \"Hi, how are you?\"]]\n",
    "\n",
    "# Translate the first input sentence then calucate the BLEU metric for translation quality\n",
    "translated_output = translator(input_sentence_1, clean_up_tokenization_spaces=True)\n",
    "\n",
    "translated_sentence = translated_output[0]['translation_text']\n",
    "\n",
    "print(\"Translated:\", translated_sentence)\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "results = bleu.compute(predictions=[translated_sentence], references=reference_1)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "86db2dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey, how are you?', \"I'm great, thanks.\"]\n",
      "{'bleu': 0.8627788640890415, 'precisions': [0.9090909090909091, 0.8888888888888888, 0.8571428571428571, 0.8], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 11, 'reference_length': 11}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import evaluate\n",
    "\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
    "\n",
    "input_sentences_2 = [\"Hola, ¿cómo estás?\", \"Estoy genial, gracias.\"]\n",
    "\n",
    "references_2 = [\n",
    "  [\"Hello, how are you?\", \"Hi, how are you?\"],\n",
    "  [\"I'm great, thanks.\", \"I'm great, thank you.\"]\n",
    "]\n",
    "\n",
    "# Translate the input sentences, extract the translated text, and compute BLEU score\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
    "\n",
    "translated_outputs = translator(input_sentences_2, clean_up_tokenization_spaces=True)\n",
    "\n",
    "predictions = [translated_output['translation_text'] for translated_output in translated_outputs]\n",
    "print(predictions)\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "results = bleu.compute(predictions=predictions, references=references_2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f261bce8",
   "metadata": {},
   "source": [
    "# Metrics for language tasks: ROUGE, METEOR,EM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4136d8c",
   "metadata": {},
   "source": [
    "## Evaluating with ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "21e4554a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE results:  {'rouge1': np.float64(0.7719298245614034), 'rouge2': np.float64(0.6181818181818182), 'rougeL': np.float64(0.736842105263158), 'rougeLsum': np.float64(0.736842105263158)}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load the rouge metric\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "predictions = [\"\"\"Pluto is a dwarf planet in our solar system, located in the Kuiper Belt beyond Neptune, and was formerly considered the ninth planet until its reclassification in 2006.\"\"\"]\n",
    "references = [\"\"\"Pluto is a dwarf planet in the solar system, located in the Kuiper Belt beyond Neptune, and was previously deemed as a planet until it was reclassified in 2006.\"\"\"]\n",
    "\n",
    "# Calculate the rouge scores between the predicted and reference summaries\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "print(\"ROUGE results: \", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30d7e28",
   "metadata": {},
   "source": [
    "## Evaluating with METEOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2e1a6223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meteor:  0.37180012567275916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jorge/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/jorge/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jorge/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "\n",
    "generated = [\"The burrow stretched forward like a narrow corridor for a while, then plunged abruptly downward, so quickly that Alice had no chance to stop herself before she was tumbling into an extremely deep shaft.\"]\n",
    "reference = [\"The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well.\"]\n",
    "\n",
    "# Compute and print the METEOR score\n",
    "results = meteor.compute(predictions=generated, references=reference)\n",
    "print(\"Meteor: \", results['meteor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751ab26d",
   "metadata": {},
   "source": [
    "## Evaluating with EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3f49fba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM results:  {'exact_match': np.float64(0.25)}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load the metric\n",
    "exact_match = evaluate.load(\"exact_match\")\n",
    "\n",
    "predictions = [\"It's a wonderful day\", \"I love dogs\", \"DataCamp has great AI courses\", \"Sunshine and flowers\"]\n",
    "references = [\"What a wonderful day\", \"I love cats\", \"DataCamp has great AI courses\", \"Sunsets and flowers\"]\n",
    "\n",
    "# Compute the exact match and print the results\n",
    "results = exact_match.compute(references=references, predictions=predictions)\n",
    "print(\"EM results: \", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce1bbb4",
   "metadata": {},
   "source": [
    "# Safeguarding LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06d19d5",
   "metadata": {},
   "source": [
    "## Checking toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f93f5861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:evaluate.loading:Using the latest cached version of the module from /home/jorge/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-measurement--toxicity/2390290fa0bf6d78480143547c6b08f3d4f8805b249df8c7a8e80d0ce8e3778b (last modified on Sat Apr 20 16:57:47 2024) since it couldn't be found locally at evaluate-measurement--toxicity, or remotely on the Hugging Face Hub.\n",
      "WARNING:evaluate_modules.metrics.evaluate-measurement--toxicity.2390290fa0bf6d78480143547c6b08f3d4f8805b249df8c7a8e80d0ce8e3778b.toxicity:Using default facebook/roberta-hate-speech-dynabench-r4-target checkpoint\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicities (user_1): [0.00013486359966918826, 0.00013348401989787817]\n",
      "Toxicities (user_2):  [0.0001355926360702142, 0.00013771136582363397]\n",
      "Maximum toxicity (user_1): 0.00013486359966918826\n",
      "Maximum toxicity (user_2):  0.00013771136582363397\n",
      "Toxicity ratio (user_1): 0.0\n",
      "Toxicity ratio (user_2):  0.0\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "toxicity_metric = evaluate.load(\"toxicity\")\n",
    "\n",
    "user_1 = ['Everyone that tried it love it', 'This artist is a true genius, pure talent']\n",
    "user_2 = [\"Nobody i've talked to likes this product\", 'Terrible singer']\n",
    "\n",
    "# Calculate the individual toxicities\n",
    "toxicity_1 = toxicity_metric.compute(predictions=user_1)\n",
    "toxicity_2 = toxicity_metric.compute(predictions=user_2)\n",
    "print(\"Toxicities (user_1):\", toxicity_1['toxicity'])\n",
    "print(\"Toxicities (user_2): \", toxicity_2['toxicity'])\n",
    "\n",
    "# Calculate the maximum toxicities\n",
    "toxicity_1_max = toxicity_metric.compute(predictions=user_1, aggregation=\"maximum\")\n",
    "toxicity_2_max = toxicity_metric.compute(predictions=user_2, aggregation=\"maximum\")\n",
    "print(\"Maximum toxicity (user_1):\", toxicity_1_max['max_toxicity'])\n",
    "print(\"Maximum toxicity (user_2): \", toxicity_2_max['max_toxicity'])\n",
    "\n",
    "# Calculate the toxicity ratios\n",
    "toxicity_1_ratio = toxicity_metric.compute(predictions=user_1, aggregation=\"ratio\")\n",
    "toxicity_2_ratio = toxicity_metric.compute(predictions=user_2, aggregation=\"ratio\")\n",
    "print(\"Toxicity ratio (user_1):\", toxicity_1_ratio['toxicity_ratio'])\n",
    "print(\"Toxicity ratio (user_2): \", toxicity_2_ratio['toxicity_ratio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844872f0",
   "metadata": {},
   "source": [
    "## Evalating regard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "45c9bc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:evaluate.loading:Using the latest cached version of the module from /home/jorge/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-measurement--regard/49c8ca499f140affc8972ee0478a52401e4537b1ebde0d486418fea1d4504625 (last modified on Sat Apr 20 16:58:39 2024) since it couldn't be found locally at evaluate-measurement--regard, or remotely on the Hugging Face Hub.\n",
      "Device set to use cpu\n",
      "WARNING:evaluate.loading:Using the latest cached version of the module from /home/jorge/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-measurement--regard/49c8ca499f140affc8972ee0478a52401e4537b1ebde0d486418fea1d4504625 (last modified on Sat Apr 20 16:58:39 2024) since it couldn't be found locally at evaluate-measurement--regard, or remotely on the Hugging Face Hub.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity in group 1:\n",
      " {'regard': [[{'label': 'neutral', 'score': 0.9586172103881836}, {'label': 'negative', 'score': 0.020242007449269295}, {'label': 'positive', 'score': 0.014409107156097889}, {'label': 'other', 'score': 0.00673176534473896}], [{'label': 'neutral', 'score': 0.8816481828689575}, {'label': 'positive', 'score': 0.08354270458221436}, {'label': 'negative', 'score': 0.01981654018163681}, {'label': 'other', 'score': 0.014992612414062023}]]}\n",
      "Polarity in group 2:\n",
      " {'regard': [[{'label': 'negative', 'score': 0.9745951890945435}, {'label': 'other', 'score': 0.017152613028883934}, {'label': 'neutral', 'score': 0.007746340706944466}, {'label': 'positive', 'score': 0.0005058045499026775}], [{'label': 'neutral', 'score': 0.7666087746620178}, {'label': 'negative', 'score': 0.10047445446252823}, {'label': 'positive', 'score': 0.07146849483251572}, {'label': 'other', 'score': 0.061448272317647934}]]}\n",
      "Polarity comparison between groups:\n",
      " {'regard_difference': {'neutral': 0.5329551389440894, 'negative': -0.5175055479630828, 'positive': 0.012988756177946925, 'other': -0.028438253793865442}}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load the regard and regard-comparison metrics\n",
    "regard = evaluate.load(\"regard\")\n",
    "regard_comp = evaluate.load(\"regard\", \"compare\")\n",
    "\n",
    "group1 = ['abc are described as loyal', 'abc are ambitious']\n",
    "group2 = ['abc are known for being confrontational', 'abc are very blunt']\n",
    "\n",
    "# Compute the regard (polarities) of each group separately\n",
    "polarity_results_1 = regard.compute(data=group1) \n",
    "print(\"Polarity in group 1:\\n\", polarity_results_1)\n",
    "polarity_results_2 = regard.compute(data=group2) \n",
    "print(\"Polarity in group 2:\\n\", polarity_results_2)\n",
    "\n",
    "# Compute the relative regard between the two groups for comparison\n",
    "polarity_results_comp = regard_comp.compute(data=group1, references=group2)\n",
    "print(\"Polarity comparison between groups:\\n\", polarity_results_comp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging-face-fundamentals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
