{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09ad1c36",
   "metadata": {},
   "source": [
    "# Evaluating LLM performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b136c4",
   "metadata": {},
   "source": [
    "## Loading metrics with evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4db3801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load the metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49d81ba",
   "metadata": {},
   "source": [
    "## Describing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98caa856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:\n",
      "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
      " Where:\n",
      "TP: True positive\n",
      "TN: True negative\n",
      "FP: False positive\n",
      "FN: False negative\n",
      "\n",
      "\n",
      "Precision is the fraction of correctly labeled positive examples out of all of the examples that were labeled as positive. It is computed via the equation:\n",
      "Precision = TP / (TP + FP)\n",
      "where TP is the True positives (i.e. the examples correctly labeled as positive) and FP is the False positive examples (i.e. the examples incorrectly labeled as positive).\n",
      "\n",
      "\n",
      "Recall is the fraction of the positive examples that were correctly labeled by the model as positive. It can be computed with the equation:\n",
      "Recall = TP / (TP + FN)\n",
      "Where TP is the true positives and FN is the false negatives.\n",
      "\n",
      "\n",
      "The F1 score is the harmonic mean of the precision and recall. It can be computed with the equation:\n",
      "F1 = 2 * (precision * recall) / (precision + recall)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Obtain a description of each metric\n",
    "print(accuracy.description)\n",
    "print(precision.description)\n",
    "print(recall.description)\n",
    "print(f1.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "650fc347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The required data types for accuracy are: {'predictions': Value('int32'), 'references': Value('int32')}.\n",
      "The required data types for precision are: {'predictions': Value('int32'), 'references': Value('int32')}.\n",
      "The required data types for recall are: {'predictions': Value('int32'), 'references': Value('int32')}.\n",
      "The required data types for f1 are: {'predictions': Value('int32'), 'references': Value('int32')}.\n"
     ]
    }
   ],
   "source": [
    "# See the required data types\n",
    "print(f\"The required data types for accuracy are: {accuracy.features}.\")\n",
    "print(f\"The required data types for precision are: {precision.features}.\")\n",
    "print(f\"The required data types for recall are: {recall.features}.\")\n",
    "print(f\"The required data types for f1 are: {f1.features}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558614aa",
   "metadata": {},
   "source": [
    "## Using evaluate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08038ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load a pretrained model and tokenizer (this is an example using a sentiment analysis model)\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Prepare some example text data\n",
    "texts = [\"I love this movie!\", \"This was a terrible experience.\", \"The food was okay.\"]\n",
    "\n",
    "# Tokenize the inputs\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Get the model outputs\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Now outputs.logits can be used with torch.argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c62d8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "# Extract the new predictions\n",
    "predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "\n",
    "print(predicted_labels)\n",
    "\n",
    "validate_labels = [1, 0, 1]  # Example true labels for the validation set\n",
    "\n",
    "# Compute the metrics by comparing real and predicted labels\n",
    "print(accuracy.compute(references=validate_labels, predictions=predicted_labels))\n",
    "print(precision.compute(references=validate_labels, predictions=predicted_labels))\n",
    "print(recall.compute(references=validate_labels, predictions=predicted_labels))\n",
    "print(f1.compute(references=validate_labels, predictions=predicted_labels))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging-face-fundamentals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
